import os
import cv2
import face_recognition
import numpy as np
import pyttsx3  # Text-to-speech library
import ctypes   # For screen dimensions and window positioning
import matplotlib.pyplot as plt
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import plot_model
from sklearn.metrics import classification_report

# Initialize TTS engine
tts_engine = pyttsx3.init()

def speak_name(name):
    """Function to say the detected name out loud."""
    tts_engine.say(name)
    tts_engine.runAndWait()

# Center the camera window on the screen
def center_window(window_name, width=640, height=480):
    """Center the OpenCV window on the screen."""
    user32 = ctypes.windll.user32
    screen_width = user32.GetSystemMetrics(0)
    screen_height = user32.GetSystemMetrics(1)

    x = (screen_width - width) // 2
    y = (screen_height - height) // 2

    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
    cv2.resizeWindow(window_name, width, height)
    cv2.moveWindow(window_name, x, y)

# Load known faces and their encodings
def load_known_faces_and_encodings(base_path):
    encodings = []
    names = []

    for person_name in os.listdir(base_path):
        person_folder = os.path.join(base_path, person_name)

        if not os.path.isdir(person_folder):
            continue

        for image_file in os.listdir(person_folder):
            image_path = os.path.join(person_folder, image_file)
            try:
                image = face_recognition.load_image_file(image_path)
                image_encodings = face_recognition.face_encodings(image)
                for encoding in image_encodings:
                    encodings.append(encoding)
                    names.append(person_name)
            except Exception as e:
                print(f"Error processing {image_path}: {e}")

    return encodings, names

# Function to create and print the CNN model
def create_cnn_model(num_classes):
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))  # Adjust input shape as needed
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(128, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))  # Adjust output layer based on number of classes
    return model

# Load all known faces and their encodings
base_path = "C:/DSP/images"
known_encodings, known_names = load_known_faces_and_encodings(base_path)

# Create CNN model and print the summary
cnn_model = create_cnn_model(len(set(known_names)))

# Compile the model
cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Simulate training data (replace with real data if available)
X_train = np.random.rand(100, 64, 64, 3)  # Dummy data
y_train = np.random.randint(0, len(set(known_names)), 100)
X_val = np.random.rand(20, 64, 64, 3)  # Dummy validation data
y_val = np.random.randint(0, len(set(known_names)), 20)

# Track and plot epoch metrics
history = cnn_model.fit(
    X_train, y_train, 
    epochs=10, 
    validation_data=(X_val, y_val)
)

# Plot accuracy and loss
def plot_training_history(history):
    epochs = range(1, len(history.history['accuracy']) + 1)

    plt.figure(figsize=(12, 5))

    # Plot Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')
    plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Accuracy over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot Loss
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history.history['loss'], label='Training Loss')
    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')
    plt.title('Loss over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Save and display the CNN model architecture with higher dpi for clarity
plot_model(cnn_model, to_file='cnn_model.png', show_shapes=True, show_layer_names=True, dpi=150)

# Display the model architecture image
img = plt.imread('cnn_model.png')
plt.figure(figsize=(15, 15))
plt.imshow(img)
plt.axis('off')
plt.show()

# Plot training metrics
plot_training_history(history)

# Initialize variables for metrics tracking
y_true = []  # Ground truth labels
y_pred = []  # Predicted labels


# Initialize camera
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

if not cap.isOpened():
    print("\nError: Camera not accessible.")
    exit()
else:
    print("\nPress 'q' to quit.")

recognized_name = "Unknown"
previous_name = "Unknown"
timeout_frames = 30
frames_since_last_seen = 0

window_name = 'Face Recognition'
center_window(window_name)

while True:
    ret, frame = cap.read()
    if not ret:
        print("Error: Unable to read frame.")
        break

    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    face_locations = face_recognition.face_locations(rgb_frame)
    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)

    name_found = False

    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):
        matches = face_recognition.compare_faces(known_encodings, face_encoding, tolerance=0.4)
        face_distances = face_recognition.face_distance(known_encodings, face_encoding)
        
        best_match_index = np.argmin(face_distances)
        if matches[best_match_index]:
            recognized_name = known_names[best_match_index]
            frames_since_last_seen = 0
            name_found = True
            y_pred.append(recognized_name)  # Predicted label
            y_true.append(recognized_name)  # Ground truth label

            confidence = (1 - face_distances[best_match_index]) * 100
            
            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
            
            # Display recognized name
            text_size, _ = cv2.getTextSize(recognized_name, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)
            text_x = left + (right - left - text_size[0]) // 2
            cv2.putText(frame, recognized_name, (text_x, top - 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)  # Move up for space

            # Display confidence with some space below the name
            confidence_text = f"{confidence:.2f}%"
            confidence_size, _ = cv2.getTextSize(confidence_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)
            confidence_x = left + (right - left - confidence_size[0]) // 2
            cv2.putText(frame, confidence_text, (confidence_x, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)  # Center confidence below name


            if recognized_name != previous_name:
                print(f"\nDetected: {recognized_name} ({confidence:.2f}%)")
                speak_name(f"Hello, {recognized_name}!")
                previous_name = recognized_name

        else:
            recognized_name = "Unknown"

    if not name_found:
        frames_since_last_seen += 1

    if frames_since_last_seen > timeout_frames:
        recognized_name = "Unknown"
        if previous_name != recognized_name:
            print("\nDetected: Unknown")
            speak_name("I don't recognize you.")
            previous_name = recognized_name

    cv2.imshow(window_name, frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

# Calculate performance metrics and print the classification report
if y_true and y_pred:  # Check if there are any predictions
    # Generate the classification report with zero_division set to 1
    text = "CLASSIFICATION REPORT"
    width = 60
    centered_text = text.center(width)
    print("\n")
    print(centered_text)
    print("\n")
    print(classification_report(y_true, y_pred))